UID,title,authors,abstract
WS01,Emulating Human Decision-Making Under Multiple Constraints,"Farzin Ahmadi, Tinglong Dai, and Kimia Ghobadi (Johns Hopkins University)","In many real-world environments, the details of decision-making processes are not fully known, e.g., how oncologists decide on specific radiation therapy treatment plans for cancer patients, how clinicians decide on medication dosages for different patients, or how hypertension patients choose their diet to control their illness. While conventional machine learning and statistical methods can be used to better understand such processes, they often fail to provide meaningful insights into the unknown parameters when the problem's setting is heavily constrained. Similarly, conventional constrained inference models, such as inverse optimization, are not well equipped for data-driven problems. In this study, we develop a novel methodology (called MLIO) that combines machine learning and inverse optimization techniques to recover the utility functions of a black-box decision-making process. Our method can be applied to settings where different types of data are required to capture the problem. MLIO is specifically developed with data-intensive medical decision-making environments in mind. We evaluate our approach in the context of personalized diet recommendations for patients, building on a large dataset of historical daily food intakes of patients from NHANES. MLIO considers these prior dietary behaviors in addition to complementary data (e.g., demographics and preexisting conditions) to recover the underlying criteria that the patients had in mind when deciding on their food choices. Once the underlying criteria are known, an optimization model can be used to find personalized diet recommendations that adhere to patients' behavior while meeting all required dietary constraints."
WS02,Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint,"Ella Y. Wang (BASIS Chandler); Anirudh Som (SRI International); Ankita Shukla, Hongjun Choi, and Pavan Turaga (ASU)","Deep neural networks have increasingly been used as an auxiliary tool in healthcare applications, due to their ability to improve performance of several diagnosis tasks. However, these methods are not widely adopted in clinical settings due to the practical limitations in the reliability, generalizability, and interpretability of deep learning based systems. As a result, methods have been developed that impose additional constraints during network training to gain more control as well as improve interpretabilty, facilitating their acceptance in healthcare community. In this work, we investigate the benefit of using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases from chest X-ray images. The OS constraint can be written as a simple orthonormality term which is used in conjunction with the standard cross-entropy loss during classification network training. Previous studies have demonstrated significant benefits in applying such constraints to deep learning models. Our findings corroborate these observations, indicating that the orthonormality loss function effectively produces improved semantic localization via GradCAM visualizations, enhanced classification performance, and reduced model calibration error. Our approach achieves an improvement in accuracy of 1.6% and 4.8% for two- and three-class classification, respectively; similar results are found for models with data augmentation applied. In addition to these findings, our work also presents a new application of the OS regularizer in healthcare, increasing the post-hoc interpretability and performance of deep learning models for COVID-19 classification to facilitate adoption of these methods in clinical settings. We also identify the limitations of our strategy that can be explored for further research in future."
WS03,Automated Meta-Analysis in Medical Research: A Causal Learning Perspective,"Lu Cheng (Arizona State University); Dmitriy Katz-Rogozhnikov, Kush R. Varshney, and Ioana Baldini (IBM Research)","Meta-analysis is a systematic approach for understanding a phenomenon by analyzing the results of many previously published experimental studies related to the same treatment and outcome measurement. It is an important tool for medical researchers and clinicians to derive reliable conclusions regarding the overall effect of treatments and interventions (e.g., drugs) on a certain outcome (e.g., the severity of a disease). Unfortunately, conventional meta-analysis involves great human effort, i.e., it is constructed by hand and is extremely time-consuming and labor-intensive, rendering a process that is inefficient in practice and vulnerable to human bias. To overcome these challenges, we work toward automating meta-analysis with a focus on controlling for the potential biases. Automating meta-analysis consists of two major steps: (1) extracting information from scientific publications written in natural language, which is different and noisier than what humans typically extract when conducting a meta-analysis; and (2) modeling meta-analysis, from a novel \textit{causal-inference} perspective, to control for the potential biases and summarize the treatment effect from the outputs of the first step. Since sufficient prior work exists for the first step, this study focuses on the second step. The core contribution of this work is a multiple causal inference algorithm tailored to the potentially noisy and biased information automatically extracted by current natural language processing systems. Empirical evaluations on both synthetic and semi-synthetic data show that the proposed approach for automated meta-analysis yields high-quality performance."
WS04,The Benefit of Distraction: Denoising Remote Vitals Measurements using Inverse Attention,Ewa Nowara (RICE UNIVERSITY); Daniel McDuff (Microsoft Research); Ashok Veeraraghavan (RICE UNIVERSITY),"Attention is a powerful concept in computer vision. End-to-end networks that learn to focus selectively on regions of an image or video often perform strongly. However, other image regions, while not necessarily containing the signal of interest, may contain useful context. We present an approach that exploits the idea that statistics of noise may be shared between the regions that contain the signal of interest and those that do not. Our technique uses the inverse of an attention mask to generate a noise estimate that is then used to denoise temporal observations. We apply this to the task of camera-based physiological measurement. A convolutional attention network is used to learn which regions of a video contain the physiological signal and generate a preliminary estimate. A noise estimate is obtained by using the pixel intensities in the inverse regions of the learned attention mask, this in turn is used to refine the estimate of the physiological signal. We perform experiments on two large benchmark datasets and show that this approach produces state-of-the-art results, increasing the signal-to-noise ratio by up to 5.8 dB, reducing heart rate and breathing rate estimation error by as much as 30%, recovering subtle waveform dynamics, and generalizing from RGB to NIR videos without retraining."
WS05,DynEHR: Dynamic Adaptation of Models with Data Heterogeneity in Electronic Health Records,"Lida Zhang (Texas A&M University); Xiaohan Chen, Tianlong Chen, and Zhangyang Wang (University of Texas at Austin); Bobak J. Mortazavi (Texas A&M University)","Electronic health records (EHRs) provide an abundance of data for clinical outcomes modeling. The prevalence of EHR data has enabled a number of studies using a variety of machine learning algorithms to predict potential adverse events. However, these studies do not account for the heterogeneity present in EHR data, including various lengths of stay, various frequencies of vitals captured in invasive versus non-invasive fashion, and various repetitions (or lack of thereof) of laboratory examinations. Therefore, studies limit the types of features extracted or the domain considered to provide a more homogeneous training set to machine learning models. The heterogeneity in this data represents important risk differences in each patient. In this work, we examine such data in an intensive care unit (ICU) setting, where the length of stay and the frequency of data gathered may vary significantly based upon the severity of patient condition. Therefore, it is unreasonable to use the same model for patients first entering the ICU versus those that have been there for above average lengths of stay. Developing multiple individual models to account for different patient cohorts, different lengths of stay, and different sources for key vital sign data may be tedious and not account for rare cases well. We address this challenge by developing a dynamic model, based upon meta-learning, to adapt to data heterogeneity and generate predictions of various outcomes across the different lengths of data. We compare this technique against a set of benchmarks on a publicly-available ICU dataset (MIMIC-III) and demonstrate improved model performance by accounting for data heterogeneity."
WS06,Active Learning for Medical Code Assignment,"Martha Ferreira (Dalhousie University); Michal Malyska and Nicola Sahar (Semantic Health); Riccardo Miotto (Icahn School of Medicine at Mount Sinai); Fernando Paulovich (Dalhousie University); Evangelos Milios (Dalhousie University, Faculty of Computer Scienc)","Machine Learning (ML) is widely used to automatically extract meaningful information from Electronic Health Records (EHR) to support operational, clinical, and financial decision making. However, ML models require a large number of annotated examples to provide satisfactory results, which is not possible in most healthcare scenarios due to the high cost of clinician labeled data. Active Learning (AL) is a process of selecting the most informative instances to be labeled by an expert to further train a supervised algorithm. We demonstrate the effectiveness of AL in multi-label text classification in the clinical domain. In this context, we apply a set of well-known AL methods to help automatically assign ICD-9 codes on the MIMIC-III dataset. Our results show that the selection of informative instances provides satisfactory classification with a significantly reduced training set (8.3\% of the total instances). We conclude that AL methods can significantly reduce the manual annotation cost while preserving model performance."
WS07,Framing Social Contact Networks for Contagion Dynamics,"Kirti Jain (Department of Computer Science, University of Delhi, Delhi, India); Sharanjit Kaur (Acharya Narendra Dev College, University of Delhi, Delhi, India); Vasudha Bhatnagar (Department of Computer Science, University of Delhi, Delhi, India)","Assessment of COVID-19 pandemic predictions indicates that differential equation-based epidemic spreading models are less than satisfactory in the contemporary world of intense human connectivity. Network-based simulations are more apt for studying the contagion dynamics due to their ability to model heterogeneity of human interactions. However, the quality of predictions in network-based models depends on how well the underlying wire-frame approximates the real social contact network of the population. In this paper, we propose a framework to create a modular wire-frame to mimic the social contact network of geography by lacing it with demographic information. The proposed inter-connected network sports small-world topology, accommodates density variations in the geography, and emulates human interactions in family, social, and work spaces. The resulting wire-frame is a generic and potent instrument for urban planners, demographers, economists, and social scientists to simulate different ""what-if"" scenarios and predict epidemic variables. The basic frame can be laden with any economic, social, urban data that can potentially shape human connectance. We present a preliminary study of the impact of variations in contact patterns due to density and demography on the epidemic variables."
WS08,CoSIR: Managing an Epidemic via Optimal Adaptive Control of Transmission Rate Policy,Harsh Maheshwari and Shreyas Shetty (Flipkart Internet Private Ltd.); Nayana Bannur (Wadhwani AI); Srujana Merugu (Independent),"Shaping an epidemic with an adaptive contact restriction policy that balances the disease and socioeconomic impact has been the holy grail during the COVID-19 pandemic. Most of the existing work on epidemiological models focuses on scenario-based forecasting via simulation but techniques for explicit control of epidemics via an analytical framework are largely missing. In this paper, we consider the problem of determining the optimal control policy for transmission rate assuming SIR dynamics, which is the most widely used epidemiological paradigm. We first demonstrate that the SIR model with infectious patients and susceptible contacts (i.e., product of transmission rate and susceptible population) interpreted as predators and prey respectively reduces to a Lotka-Volterra (LV) predator-prey model. The modified SIR system (LVSIR) has a stable equilibrium point, an 'energy' conservation property, and exhibits bounded cyclic behaviour similar to an LV system. This mapping permits a theoretical analysis of the control problem supporting some of the recent simulation-based studies that point to the benefits of periodic interventions. We use a control-Lyapunov approach to design adaptive control policies (CoSIR) to nudge the SIR model to the desired equilibrium that permits ready extensions to richer compartmental models. We also describe a practical implementation of this transmission control method by approximating the ideal control with a finite, but a time-varying set of restriction levels. We provide experimental results comparing with periodic lockdowns on few different geographical regions (India, Mexico, Netherlands) to demonstrate the efficacy of this approach."
WS09,CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays,"Emma Chen, Andy Kim, Rayan Krishnan, Andrew Y. Ng, and Pranav Rajpurkar (Stanford University)","A major obstacle to the integration of deep learning models for chest x-ray interpretation into clinical settings is the lack of understanding of their failure modes. In this work, we first investigate whether there are clinical subgroups that chest x-ray models are likely to misclassify. We find that older patients and patients with a lung lesion or pneumothorax finding have a higher probability of being misclassified on some diseases. Second, we develop misclassification predictors on chest x-ray models using their outputs and clinical features. We find that our best performing misclassification identifier achieves an AUROC close to 0.9 for most diseases. Third, employing our misclassification identifiers, we develop a corrective algorithm to selectively flip model predictions that have high likelihood of misclassification at inference time. We observe F1 improvement on the prediction of Consolidation (0.008, 95%CI[0.005, 0.010]) and Edema (0.003, 95%CI[0.001, 0.006]). By carrying out our investigation on ten distinct and high-performing chest x-ray models, we are able to derive insights across model architectures and offer a generalizable framework applicable to other medical imaging tasks."
WS10,MoCo-CXR: MoCo Pretraining Improves Representation and Transferability of Chest X-ray Models,"Hari Sowrirajan, Jingbo Yang, Andrew Ng, and Pranav Rajpurkar (Stanford University)","Contrastive learning is a form of self-supervision that can leverage unlabeled data to produce pretrained models. While contrastive learning has demonstrated promising results on natural image classification tasks, its application to medical imaging tasks like chest X-ray interpretation has been limited. In this work, we propose MoCo-CXR, which is an adaptation of the contrastive learning method Momentum Contrast (MoCo), to produce models with better representations and initializations for the detection of pathologies in chest X-rays. In detecting pleural effusion, we find that linear models trained on MoCo-CXR-pretrained representations outperform those without MoCo-CXR-pretrained representations, indicating that MoCo-CXR-pretrained representations are of higher-quality. End-to-end fine-tuning experiments reveal that a model initialized via MoCo-CXR-pretraining outperforms its non-MoCo-CXR-pretrained counterpart. We find that MoCo-CXR-pretraining provides the most benefit with limited labeled training data. Finally, we demonstrate similar results on a target Tuberculosis dataset unseen during pretraining, indicating that MoCo-CXR-pretraining endows models with representations and transferability that can be applied across chest X-ray datasets and tasks."
WS11,Encoding physical conditioning from inertial sensors for multi-step heart rate estimation,"Davi Pedrosa de Aguiar, Otávio Augusto Silva, and Fabricio Murai (Universidade Federal de Minas Gerais)","Inertial Measurement Unit (IMU) sensors are becoming increasingly ubiquitous in everyday devices such as smartphones, fitness watches, etc. As a result, the array of health-related applications that tap onto this data has been growing, as well as the importance of designing accurate prediction models for tasks such as human activity recognition (HAR). However, one important task that has received little attention is the prediction of an individual's heart rate when undergoing a physical activity using IMU data. This could be used, for example, to determine which activities are safe for a person without having him/her actually perform them. We propose a neural architecture for this task composed of convolutional and LSTM layers, similarly to the state-of-the-art techniques for the closely related task of HAR. However, our model includes a convolutional network that extracts, based on sensor data from a previously executed activity, a physical conditioning embedding (PCE) of the individual to be used as the LSTM's initial hidden state. We evaluate the proposed model, dubbed PCE-LSTM, when predicting the heart rate of 23 subjects performing a variety of physical activities from IMU-sensor data available in public datasets (PAMAP2, PPG-DaLiA). For comparison, we use as baselines the only model specifically proposed for this task, and an adapted state-of-the-art model for HAR. PCE-LSTM yields over 10% lower mean absolute error. We demonstrate empirically that this error reduction is in part due to the use of the PCE. Last, we use the two datasets (PPG-DaLiA, WESAD) to show that PCE-LSTM can also be successfully applied when photoplethysmography (PPG) sensors are available to rectify heart rate measurement errors caused by movement, outperforming the state-of-the-art deep learning baselines by more than 30%."
WS12,Towards Reliable and Trustworthy Computer-Aided Diagnosis Predictions: Diagnosing COVID-19 from X-Ray Images,"Krishanu Sarker (Georgia State University); Sharbani Pandit (Georgia Institute of Technology); Anupam Sarker (Institute of Epidemiology, Disease Control and Research); Saeid Belkasim and Shihao Ji (Georgia State University)","COVID-19 pandemic has been ravaging the world we know since it's insurgence. Computer-Aided Diagnosis (CAD) systems with high precision and reliability can play a vital role in the battle against COVID-19. Most of the existing works in the literature focus on developing sophisticated methods yielding high detection performance yet not addressing the issue of predictive uncertainty. Uncertainty estimation has been explored heavily in the literature for Deep Neural Networks; however, not much work focused on this issue on COVID-19 detection. In this work, we explore the efficacy of state-of-the-art (SOTA) uncertainty estimation methods on COVID-19 detection. We propose to augment the best performing method by using feature denoising algorithm to gain higher Positive Predictive Value (PPV) on COVID positive cases. Through extensive experimentation, we identify the most lightweight and easy-to-deploy uncertainty estimation framework that can effectively identify the confusing COVID-19 cases for expert analysis while performing comparatively with the existing resource heavy uncertainty estimation methods. In collaboration with medical professionals, we further validate the results to ensure the viability of the framework in clinical practice."
WS13,CheXseen: Unseen Disease Detection for Deep Learning Interpretation of Chest X-rays,"Siyu Shi (Department of Medicine, School of Medicine, Stanford University); Ishaan Malhi, Kevin Tran, Andrew Y. Ng, and Pranav Rajpurkar (Department of Computer Science, Stanford University)","We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) as ""no disease"". Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes."
WS14,A Data-Driven Approach to Estimating Infectious Disease Transmission from Graphs: A Case of Class Imbalance Driven Low Homophily,"Jeeheh Oh (University of Michigan, Ann Arbor); Jenna Wiens (University of Michigan)","We explore the application of graph neural networks (GNNs) to the problem of estimating exposure to an infectious pathogen and probability of transmission. Specifically, given a datatset in which a subset of patients are known to be infected and information in the form of a graph about who has interacted with whom, we aim to directly estimate transmission dynamics, i.e., what types of interactions (e.g., length and number) lead to transmission events. While, graph neural networks (GNNs) have proven capable of learning meaningful representations from graph data, they commonly assume tasks with high homophily (i.e., nodes that share an edge look similar). Recently researchers have proposed techniques for addressing problems with low homophily (e.g., adding residual connections to GNNs). In our problem setting, homophily is high on average, the majority of patients do not become infected. But, homophily remains low with respect to the minority class. In this paper, we characterize this setting as particularly challenging for GNNs. Given the asymmetry in homophily between classes, we hypothesize that solutions designed to address low homophily on average will not suffice and instead propose a solution based on attention. Applied to both real-world and synthetic network data, we test this hypothesis and explore the ability of GNNs to learn complex transmission dynamics directly from network data. Overall, attention proves to be an effective mechanism for addressing low homophily in the minority class (AUROC with 95\% CI: GCN 0.684 (0.659,0.710) vs. GAT 0.715 (0.688,0.742)) and such a data-driven approach can outperform approaches based on potentially flawed expert knowledge."
WS15,Interpretable Machine Learning Prediction of All-cause Mortality,"Wei Qiu, Hugh Chen, Ayse Berceste Dincer, and Su-In Lee (Paul G. Allen School of Computer Science and Engineering, University of Washington)","Explainable artificial intelligence provides an opportunity to improve prediction accuracy over standard linear models using 'black box' machine learning (ML) models while still revealing insights into a complex outcome such as all-cause mortality. We propose the IMPACT (Interpretable Machine learning Prediction of All-Cause morTality) framework that implements and explains complex, non-linear ML models in epidemiological research, by combining a tree ensemble mortality prediction model and an explainability method. We use 133 variables from NHANES 1999-2014 datasets (number of samples: ?? = 47, 261) to predict all-cause mortality. To explain our model, we extract local (i.e., per-sample) explanations to verify well-studied mortality risk factors, and make new dis- coveries. We present major factors for predicting ??-year mortality (?? = 1, 3, 5) across different age groups and their individualized im- pact on mortality prediction. Moreover, we highlight interactions between risk factors associated with mortality prediction, which leads to findings that linear models do not reveal. We demonstrate that compared with traditional linear models, tree-based models have unique strengths such as: (1) improving prediction power, (2) making no distribution assumptions, (3) capturing non-linear relationships and important thresholds, (4) identifying feature interactions, and (5) detecting different non-linear relationships between models. Given the popularity of complex ML models in prognostic research, combining these models with explainability methods has implications for further applications of ML in medical fields. To our knowledge, this is the first study that combines complex ML models and state-of-the-art feature attributions to explain mortality prediction, which enables us to achieve higher prediction accuracy and gain new insights into the effect of risk factors on mortality."
WS16,Outcomes-Driven Clinical Phenotyping in Patients with Cardiogenic Shock for Risk Modeling and Comparative Treatment Effectiveness,Nathan C. Hurley (Texas A&M University); Alyssa Berkowitz (Yale University); Frederick Masoudi (University of Colorado School of Medicine); Joseph Ross and Nihar Desai (Yale University); Nilay Shah (Mayo Clinic); Sanket Dhruva (UCSF School of Medicine); Bobak J. Mortazavi (Texas A&M University),"Cardiogenic shock is a deadly and complicated illness. Despite extensive research into treating cardiogenic shock, mortality remains high and has not decreased over time. Patients suffering from cardiogenic shock are highly heterogeneous, and developing an understanding of phenotypes among these patients is crucial for understanding this disease and the appropriate treatments for individual patients. In this work, we develop a deep mixture of experts approach to jointly find phenotypes among patients with cardiogenic shock while simultaneously estimating their risk of in-hospital mortality. Although trained with information regarding treatment and outcomes, after training, the proposed model is decomposable into a network that clusters patients into phenotypes from information available prior to treatment. This model is validated on a synthetic dataset and then applied to a cohort of 28,304 patients with cardiogenic shock. The full model predicts in-hospital mortality on this cohort with an AUROC of 0.85 ± 0.01. The model discovers five phenotypes among the population, finding statistically different mortality rates among them and among treatment choices within those groups. This approach allows for grouping patients in clinical clusters with different rates of device utilization and different risk of mortality. This approach is suitable for jointly finding phenotypes within a clinical population and in modeling risk among that population."
WS17,Multi-Objective Model-based Reinforcement Learning for Infectious Disease Control,"Runzhe Wan, Xinyu Zhang, and Rui Song (North Carolina State University)","Severe infectious diseases such as the novel coronavirus (COVID-19) pose a huge threat to public health. Stringent control measures, such as school closures and stay-at-home orders, while having significant effects, also bring huge economic losses. In the face of an emerging infectious disease, a crucial question for policymakers is how to make the trade-off and implement the appropriate interventions timely, with the existence of huge uncertainty. In this work, we propose a Multi-Objective Model-based Reinforcement Learning framework to facilitate data-driven decision making and minimize the long-term overall cost. Specifically, at each decision point, a Bayesian epidemiological model is first learned as the environment model, and then the proposed model-based multi-objective planning algorithm is applied to find a set of Pareto-optimal policies. This framework, combined with the prediction bands for each policy, provides a real-time decision support tool for policymakers. The application is demonstrated with the spread of COVID-19 in China."
WS18,An Experimental Evaluation of Transformer-based LanguageModels in the Biomedical Domain,Paul Grouchi (Untether AI); Shobhit Jain (Manulife); Michael Liu (Tealbook); Kuhan Wang (CIBC); Max Tian (Adeptmind); Nidhi Arora (Intact); Hillary Ngai (University of Toronto); Faiza Khan Khattak (Manulife); Elham Dolatabadi and Sedef Akinli Kocak (Vector Institute),"With the growing amount of text in health data, there have beenrapid advances in large pre-trained models that can be applied to awide variety of biomedical tasks with minimal task-specific mod-ifications. Emphasizing the cost of these models, which renderstechnical replication challenging, this paper summarizes experi-ments conducted in replicating BioBERT and further pre-trainingand careful fine-tuning in the biomedical domain. We also inves-tigate the effectiveness of domain-specific and domain-agnosticpre-trained models across downstream biomedical NLP tasks. Ourfinding confirms that pre-trained models can be impactful in somedownstream NLP tasks (QA and NER) in the biomedical domain;however, this improvement may not justify the high cost of domain-specific pre-training."
WS19,Knowledge Graph-based Question Answering with Electronic Health Records,Junwoo Park and Youngwoo Cho (Korea Advanced Institute of Science and Technology (KAIST)); Haneol Lee (Yonsei University); Jaegul Choo and Edward Choi (Korea Advanced Institute of Science and Technology (KAIST)),"Question Answering (QA) is a widely-used framework for developing and evaluating an intelligent machine. In this light, QA on Electronic Health Records (EHR), namely EHR QA, can work as a crucial milestone towards developing an intelligent agent in healthcare. EHR data are typically stored in a  relational database, which can also be converted to a directed acyclic graph, allowing two approaches for EHR QA: Table-based QA and Knowledge Graph-based QA. We hypothesize that the graph-based approach is more suitable for EHR QA as graphs can represent relations between entities and values more naturally compared to tables, which essentially require JOIN operations. In this paper, we propose a graph-based EHR QA where natural language queries are converted to SPARQL instead of SQL. To validate our hypothesis, we create four EHR QA datasets (graph-based VS table-based, and simplified database schema VS original database schema), based on a table-based dataset MIMICSQL. We test both a simple Seq2Seq model and a state-of-the-art EHR QA model on all datasets where the graph-based datasets facilitated up to 34% higher accuracy than the table-based dataset without any modification to the model architectures. Finally, all datasets will be open-sourced to encourage further EHR QA research in both directions."
WS20,CATAN: Chart-aware temporal attention network for clinical text classification,Zelalem Gero and Joyce Ho (Emory University),"There is an increased adoption of electronic health record (EHR) systems by variety of hospitals and medical centers. This provides an opportunity to leverage automated computer systems in assisting healthcare workers. One of the least utilized but rich source of patient information is the unstructured clinical text. In this work, we develop \model, a chart-aware temporal attention network for learning patient representations from clinical notes. We introduce a novel representation where each note is considered a single unit, like a sentence, and composed of attention-weighted words. The notes in turn are aggregated into a patient representation using a second weighting unit, note attention. Unlike standard attention computations which focus only on the content of the note, we incorporate the chart-time for each note as a constraint for attention calculation. This allows our model to focus on notes closer to the prediction time. Using the MIMIC-III dataset, we empirically show that our patient representation and attention calculation achieves the best performance in comparison with various state-of-the-art baselines for one-year mortality prediction and 30-day hospital readmission. Moreover, the attention weights can be used to offer transparency into our model's predictions."
WS21,Deep Cox Mixtures for Survival Regression,Chirag Nagpal (Carnegie Mellon University); Steve Yadlowsky, Negar Rostamzadeh, and Katherine Heller (Google Brain),"Survival analysis is a challenging variation of regression modeling because of the presence of censoring, where the outcome measurement is only partially known, due to, for example, loss to follow up. Such problems come up frequently in medical applications, making survival analysis a key endeavor in biostatistics and machine learning for healthcare, with Cox regression models being amongst the most commonly employed models. We describe a new approach for survival analysis regression models, based on learning mixtures of Cox regressions to model individual survival distributions. We propose an approximation to the Expectation Maximization algorithm for this model that does hard assignments to mixture groups to make optimization efficient. In each group assignment, we fit the hazard ratios within each group using deep neural networks, and the baseline hazard for each mixture component non-parametrically. We perform experiments on multiple real world datasets, and look at the mortality rates of patients across ethnicity and gender. We emphasize the importance of calibration in healthcare settings and demonstrate that our approach outperforms classical and modern survival analysis baselines, both in terms of discriminative performance and calibration, with large gains in performance on the minority demographics."
