## <center> Impact and Society:<br /> Policy, Public Health, and Social Outcomes </center>

### Track Chairs
- Dr. Sanja Šćepanović
- Dr. Stephen Pfohl
- Dr. Dimitrios Spathis

### Description
Algorithms do not exist in a vacuum: instead, they often explicitly aim for important social outcomes. This track considers issues at the intersection of algorithms and the societies they seek to impact, specifically for health. Submissions could include methodological contributions such as algorithmic development and performance evaluation for policy and public health applications, large-scale or challenging data collection, combining clinical and non-clinical data, as well as detecting and measuring bias. Submissions could also include impact-oriented research such as determining how algorithmic systems for health may introduce, exacerbate, or reduce inequities and inequalities, discrimination, and unjust outcomes, as well as evaluating the economic implications of these systems.  In other words, we invite submissions tackling the responsible design of AI applications for healthcare and public health. System design for the implementation of such applications at scale is also welcome, which often requires balancing various tradeoffs in decision-making. Submissions related to understanding barriers to the deployment and adoption of algorithmic systems for societal-level health applications are also of interest. In addressing these problems, insights from social sciences, law, clinical medicine, and the humanities can be crucial.

We welcome papers from but not limited to following areas of interest: responsible AI for health, fairness, equity, ethics and justice, policy, public health, and societal impact of algorithms, interpretability, system design for implementation of ML at scale, regulatory frameworks, tools for the adoption of ML, evaluation of bias in legal and/or health contexts, and human-algorithm interaction.

Upon submission, authors will select one or more relevant sub-discipline(s). Peer reviewers for a paper will be experts in the sub-discipline(s) selected upon its submission.


### Examples
Bolukbasi, Tolga, et al. "Quantifying and reducing stereotypes in word embeddings." arXiv preprint arXiv:1606.06121 (2016).

Obermeyer, Ziad, et al. "Dissecting racial bias in an algorithm used to manage the health of populations." Science 366.6464 (2019): 447-453.

Kleinberg, Jon, and Sendhil Mullainathan. "Simplicity creates inequity: implications for fairness, stereotypes, and interpretability." Proceedings of the 2019 ACM Conference on Economics and Computation. 2019.

Zink, Anna, and Sherri Rose. "Fair regression for health care spending." Biometrics 76.3 (2020): 973-982.

Yang, Wanqian, et al. "Incorporating interpretable output constraints in Bayesian neural networks." Advances in Neural Information Processing Systems 33 (2020): 12721-12731.

Bhatt, Umang, et al. "Explainable machine learning in deployment." Proceedings of the 2020 conference on fairness, accountability, and transparency. 2020.

Pierson, Emma, et al. "An algorithmic approach to reducing unexplained pain disparities in underserved populations." Nature Medicine 27.1 (2021): 136-140.

Seyyed-Kalantari, Laleh, et al. "Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations." Nature medicine 27.12 (2021): 2176-2182.

Panch, Trishan, Heather Mattie, and Leo Anthony Celi. "The “inconvenient truth” about AI in healthcare." NPJ digital medicine 2.1 (2019): 1-3.

